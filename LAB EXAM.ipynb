{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "589df5c4-fbae-4b74-b63b-37e3683cb7fd",
      "metadata": {
        "id": "589df5c4-fbae-4b74-b63b-37e3683cb7fd"
      },
      "source": [
        "Create an independent non-linear function str(x) = sigmoid(tanh(relu(x))), without using any other functions, in the Value class you defined earlier. The new str() function must have the logic for forward pass and the logic for grad calculation (which will be used during backprop). You could use your earlier notebook for help. <br>\n",
        "Forward pass logic (4M)<br>\n",
        "Grad calculation logic (6M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "acdd219a-78e4-4a36-ae87-c7b15de8a886",
      "metadata": {
        "id": "acdd219a-78e4-4a36-ae87-c7b15de8a886"
      },
      "outputs": [],
      "source": [
        "class Value:\n",
        "    def __init__(self, data, _children=(), _op='', label=''):\n",
        "        self.data= data\n",
        "        self.grad= 0.0\n",
        "        self._prev= set(_children)\n",
        "        self._op= _op\n",
        "        self.label= label\n",
        "        self._backward= lambda: None\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Value(data= {self.data})\"\n",
        "\n",
        "    def tr(self):\n",
        "        # Your code here\n",
        "        def relu(self):\n",
        "          out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
        "\n",
        "          def _backward():\n",
        "              self.grad += (out.data > 0) * out.grad\n",
        "          out._backward = _backward\n",
        "\n",
        "          return out\n",
        "        def tanh(self):\n",
        "          x = self.data\n",
        "          t = (np.exp(2*x) - 1) / (np.exp(2*x) + 1)\n",
        "          out = Value(t, (self,), 'tanh')\n",
        "\n",
        "          def _backward():\n",
        "                self.grad += (1 - t**2) * out.grad\n",
        "          out._backward = _backward\n",
        "\n",
        "          return out\n",
        "        def sigmoid(self):\n",
        "          x = self.data\n",
        "          s = 1 / (1 + np.exp(-x))\n",
        "          out = Value(s, (self,), 'sigmoid')\n",
        "\n",
        "          def _backward():\n",
        "                self.grad += (s * (1 - s)) * out.grad\n",
        "          out._backward = _backward\n",
        "\n",
        "          return out\n",
        "\n",
        "        def str(self):\n",
        "          relu_out = self.relu()\n",
        "          tanh_out = relu_out.tanh()\n",
        "          sigmoid_out = tanh_out.sigmoid()\n",
        "          return sigmoid_out\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c87cfcbe-30d1-4c56-8553-fe6c16cfe45e",
      "metadata": {
        "id": "c87cfcbe-30d1-4c56-8553-fe6c16cfe45e"
      },
      "source": [
        "Implement <b>Causal</b> Self Attention in matrix form according to your Key Text, i.e. equation 10.15 and 10.16, Page 220.<br> Fig 10.4 (masking out the future) is very helpful for implementing <b>Causal</b> Self Attention.<br>Use the X, WQ, WK and WV matrices to compute the attention matrix A. Use only basic numpy functions like np.dot(), np.exp() and np.sum() (15M)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "b9ec4c96-8183-4394-aad1-de049ecc6cbb",
      "metadata": {
        "id": "b9ec4c96-8183-4394-aad1-de049ecc6cbb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.random.seed(123)\n",
        "#Random initialization of matrices\n",
        "X= np.random.rand(100,200) / 1000\n",
        "WQ= np.random.rand(200,150) / 1000\n",
        "WK= np.random.rand(200,150) / 1000\n",
        "WV= np.random.rand(200,150) / 1000\n",
        "#Do not edit the above lines\n",
        "#Implement the matrix multiply\n",
        "\n",
        "#Your code below\n",
        "matrix_multiply1= np.dot(X,WQ)#Q\n",
        "matrix_multiply2= np.dot(X,WK)#K\n",
        "matrix_multiply3= np.dot(X,WV)#v\n",
        "matrix_multiply2transpose= np.transpose(matrix_multiply2)#K^T\n",
        "matrix_multiply= np.dot(matrix_multiply1,matrix_multiply2transpose)#QK^T\n",
        "#dk= its used to divide by the squareroot of the dimensionality of the Q and K vectors\n",
        "matrix_divide_by_dk= np.sqrt(matrix_multiply/(matrix_multiply.shape[1]))#QK^T/sqrt(dk)\n",
        "softmax= np.exp(matrix_divide_by_dk) / np.sum(np.exp(matrix_divide_by_dk), axis=1, keepdims=True)\n",
        "Self_Attention= np.dot(softmax,matrix_multiply3)#softmax((QK^T/sqrt(dk)))*V"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vc7RDbcacxu3"
      },
      "id": "vc7RDbcacxu3",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}